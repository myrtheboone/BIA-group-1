{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 3: Convolutional neural networks for classification\n",
    "\n",
    "The goal of this assignment is to demonstrate the Keras API for implementing and training convolutional neural network architectures. Furthermore, you get to work with the PatchCAMELYON (or PCAM) dataset that you should also use for the main project work. Essentially, this assignment demonstrated a minimal working example for the main project work.\n",
    "\n",
    "### Preliminaries\n",
    "\n",
    "The full working code of the example convolutional neural network can be found in the `cnn.py` file. As before, we will go over the components of the code in this Python notebook, however, you are strongly encouraged to perform all experiments using `cnn.py`. We start with importing the required libraries and defining the size of the images in the PCAM dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Flatten\n",
    "from tensorflow.keras.layers import Conv2D, MaxPool2D\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, TensorBoard\n",
    "import keras\n",
    "from importlib import reload\n",
    "reload(keras.models)\n",
    "\n",
    "# unused for now, to be used for ROC analysis\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "\n",
    "\n",
    "# the size of the images in the PCAM dataset\n",
    "IMAGE_SIZE = 96"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instantiating data generators\n",
    "\n",
    "Compared to the MNIST dataset, the PatchCAMELYON dataset is too big to fit in the working memory of most personal computers. This is why, we need to define some functions that will read the image data batch by batch, so only a single batch of images needs to be stored in memory at one time point. We can use the handy ImageDataGenerator function from the Keras API to do this. Note that the generators are defined within a function that returns them as output arguments. This function will later be called from the main code body. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pcam_generators(base_dir, train_batch_size=32, val_batch_size=32):\n",
    "\n",
    "     # dataset parameters\n",
    "     TRAIN_PATH = os.path.join(base_dir, 'train+val', 'train')\n",
    "     VALID_PATH = os.path.join(base_dir, 'train+val', 'valid')\n",
    "\n",
    "     RESCALING_FACTOR = 1./255\n",
    "     \n",
    "     # instantiate data generators\n",
    "     datagen = ImageDataGenerator(rescale=RESCALING_FACTOR)\n",
    "\n",
    "     train_gen = datagen.flow_from_directory(TRAIN_PATH,\n",
    "                                             target_size=(IMAGE_SIZE, IMAGE_SIZE),\n",
    "                                             batch_size=train_batch_size,\n",
    "                                             class_mode='binary')\n",
    "\n",
    "     val_gen = datagen.flow_from_directory(VALID_PATH,\n",
    "                                             target_size=(IMAGE_SIZE, IMAGE_SIZE),\n",
    "                                             batch_size=val_batch_size,\n",
    "                                             class_mode='binary',\n",
    "                                             shuffle=False)\n",
    "     \n",
    "     return train_gen, val_gen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building a convolutional neural network classification model\n",
    "\n",
    "The convolutional neural network model is also defined within a function. Organizing the code into functions instead of piling everything up in a single script makes the code more clear to read and understand, and helps reuse functionality that is already implemented. For example, we can use the `get_pcam_generators()` function to create data generators with different batch sizes just by calling the function with a different set of parameters. Or, we can use the `get_model()` function to generate networks with different number of feature maps (see below). \n",
    "\n",
    "The convolutional neural network model consists of two convolutional layers, each one followed by a max pooling layer and a fully connected layer with 64 neurons. The kernel size and number of filters of the two convolutional layers, and the size of the max pooling regions can be passed as input parameters to the function (however, note that default values are set so the function can be called without parameters). ReLU nonlinearities are used throughout the network, except for the output neuron that is activated with a sigmoid. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(kernel_size=(3,3), pool_size=(4,4), first_filters=32, second_filters=64):\n",
    "\n",
    "\n",
    "     # build the model\n",
    "     model = Sequential()\n",
    "\n",
    "     model.add(Conv2D(first_filters, kernel_size, activation = 'relu', padding = 'same', input_shape = (IMAGE_SIZE, IMAGE_SIZE, 3)))\n",
    "     model.add(MaxPool2D(pool_size = pool_size)) \n",
    "\n",
    "     model.add(Conv2D(second_filters, kernel_size, activation = 'relu', padding = 'same'))\n",
    "     model.add(MaxPool2D(pool_size = pool_size))\n",
    "\n",
    "     model.add(Flatten())\n",
    "     model.add(Dense(64, activation = 'relu'))\n",
    "     model.add(Dense(1, activation = 'sigmoid'))\n",
    "     \n",
    "    \n",
    "     # compile the model\n",
    "     model.compile(SGD(learning_rate=0.01, momentum=0.95), loss = 'binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "     return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, the two functions that define the model and the data generators can be called from the main code body. Before executing the code block below, do not forget to change the path where the PatchCAMELYON dataset is located (that is, the location of the folder that contains `train+val` that you previously downloaded and unpacked).\n",
    "\n",
    "If everything is correct, the following output will be printed on screen after executing the code block:\n",
    "\n",
    "`Found 144000 images belonging to 2 classes.`\n",
    "\n",
    "`Found 16000 images belonging to 2 classes.`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 144000 images belonging to 2 classes.\n",
      "Found 16000 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "# get the model\n",
    "model = get_model()\n",
    "\n",
    "# get the data generators\n",
    "train_gen, val_gen = get_pcam_generators('C:/Users//20192823//Documents//3 jaar//Kwartiel 3//BIA')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to know the shapes of the outputs of all layers in the network (the dimensionality of the feature maps), you can print them in the following way:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(None, 96, 96, 32)\n",
      "(None, 24, 24, 32)\n",
      "(None, 24, 24, 64)\n",
      "(None, 6, 6, 64)\n",
      "(None, 2304)\n",
      "(None, 64)\n",
      "(None, 1)\n"
     ]
    }
   ],
   "source": [
    "for layer in model.layers:\n",
    "    print(layer.output_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_16 (Conv2D)           (None, 96, 96, 32)        896       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_8 (MaxPooling2 (None, 24, 24, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_17 (Conv2D)           (None, 24, 24, 64)        18496     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_9 (MaxPooling2 (None, 6, 6, 64)          0         \n",
      "_________________________________________________________________\n",
      "flatten_4 (Flatten)          (None, 2304)              0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 64)                147520    \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 65        \n",
      "=================================================================\n",
      "Total params: 166,977\n",
      "Trainable params: 166,977\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "def get_model_fullyconv(kernel_size=(3,3), pool_size=(4,4), first_filters=32, second_filters=64):\n",
    "\n",
    "\n",
    "     # build the model\n",
    "     model = Sequential()\n",
    "\n",
    "     model.add(Conv2D(first_filters, kernel_size, activation = 'relu', padding = 'same', input_shape = (IMAGE_SIZE, IMAGE_SIZE, 3)))\n",
    "     model.add(MaxPool2D(pool_size = pool_size)) \n",
    "\n",
    "     model.add(Conv2D(second_filters, kernel_size, activation = 'relu', padding = 'same'))\n",
    "     model.add(MaxPool2D(pool_size = pool_size))\n",
    "\n",
    "     model.add(Conv2D(64, (6,6), activation = 'relu'))\n",
    "     model.add(Conv2D(1, (1,1), activation = 'sigmoid'))\n",
    "     model.add(Flatten())\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "     # compile the model\n",
    "     model.compile(SGD(learning_rate=0.01, momentum=0.95), loss = 'binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "     return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(None, 96, 96, 32)\n",
      "(None, 24, 24, 32)\n",
      "(None, 24, 24, 64)\n",
      "(None, 6, 6, 64)\n",
      "(None, 1, 1, 64)\n",
      "(None, 1, 1, 1)\n",
      "(None, 1)\n"
     ]
    }
   ],
   "source": [
    "# get the model\n",
    "model_fullyconv = get_model_fullyconv()\n",
    "\n",
    "for layer in model_fullyconv.layers:\n",
    "    print(layer.output_shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_5\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_18 (Conv2D)           (None, 96, 96, 32)        896       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_10 (MaxPooling (None, 24, 24, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_19 (Conv2D)           (None, 24, 24, 64)        18496     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_11 (MaxPooling (None, 6, 6, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_20 (Conv2D)           (None, 1, 1, 64)          147520    \n",
      "_________________________________________________________________\n",
      "conv2d_21 (Conv2D)           (None, 1, 1, 1)           65        \n",
      "_________________________________________________________________\n",
      "flatten_5 (Flatten)          (None, 1)                 0         \n",
      "=================================================================\n",
      "Total params: 166,977\n",
      "Trainable params: 166,977\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_fullyconv.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training and evaluating the model\n",
    "\n",
    "Finally, the model can be trained using data generated by the data generators and then evaluated. This is done in a similar way to the previous assignment. Furthermore, in addition to the Tensorflow callback, an additional callback that saves the \"best\" version of the trained model to a file is added, and the model structure is saved to a json file. This enables loading the model and corresponding weights at a later time point (e.g. when we want to evaluate the model on a test set). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "   2/4500 [..............................] - ETA: 28:21 - loss: 0.7142 - accuracy: 0.5156WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1177s vs `on_train_batch_end` time: 0.6379s). Check your callbacks.\n",
      "4500/4500 [==============================] - ETA: 0s - loss: 0.4958 - accuracy: 0.7568\n",
      "Epoch 00001: val_loss improved from inf to 0.42315, saving model to my_first_cnn_model_weights.hdf5\n",
      "4500/4500 [==============================] - 499s 111ms/step - loss: 0.4958 - accuracy: 0.7568 - val_loss: 0.4232 - val_accuracy: 0.8054\n",
      "Epoch 2/3\n",
      "4500/4500 [==============================] - ETA: 0s - loss: 0.4194 - accuracy: 0.8095\n",
      "Epoch 00002: val_loss improved from 0.42315 to 0.40846, saving model to my_first_cnn_model_weights.hdf5\n",
      "4500/4500 [==============================] - 498s 111ms/step - loss: 0.4194 - accuracy: 0.8095 - val_loss: 0.4085 - val_accuracy: 0.8170\n",
      "Epoch 3/3\n",
      "4500/4500 [==============================] - ETA: 0s - loss: 0.3958 - accuracy: 0.8226\n",
      "Epoch 00003: val_loss did not improve from 0.40846\n",
      "4500/4500 [==============================] - 500s 111ms/step - loss: 0.3958 - accuracy: 0.8226 - val_loss: 0.4292 - val_accuracy: 0.8125\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYoAAAEKCAYAAAAMzhLIAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8/fFQqAAAACXBIWXMAAAsTAAALEwEAmpwYAAAm20lEQVR4nO3deZxcZZX/8c+p6n1JZ+nO1kkge9gSCCECsqMQkBEREcFxfrj8EBV1dFT8qT9HZdwGdUbGBSMiOjowgihhUXAZCbJIAoSsEEL2pJN0lu50p7dazvxxq5O27VQqnb61dH/fr1e/uu5St05fwj313Oc+5zF3R0RE5HAiuQ5ARETymxKFiIikpUQhIiJpKVGIiEhaShQiIpKWEoWIiKQVWqIws7vMbJeZrTzMdjOz281snZktN7O5YcUiIiL9F2aL4m5gQZrtlwHTUz83At8PMRYREemn0BKFuy8G9qbZ5Urgpx54FhhuZuPCikdERPqnKIefXQ9s6bG8NbWuofeOZnYjQauDysrK02fNmpWVAEWk8CXdcYfuGhSeWk72+B1LOpbaFk86SSdY7t4f6IoniUaM7mIW3es740mKIkbSnY5Y8NoJ3ux46hjB5+RS1451u929rj/vzWWisD7W9Xkm3X0hsBBg3rx5vnTp0jDjEpEc2d8RIxZPEks4sUSSA11x2rsS7G7torUzxvrGAxRHI3TGE7y6s5Xykig793fQ3B6ntChCPJmkM5bk1V2tVJREaetK9CuO7lst3RepoohRCiSSzphhpRRFIhRFjWjEKIoYTW0xZoyppqQoQktHjEkjKymKGNFosL07wXQlkowbVkZRNEJRxCiKGkXRCMPKiijusS4aCV7HEkmGV5RgQMQMM4IfjEgk9bt7ndlf7RexIPpIJFhfP6JiU3//u+QyUWwFJvZYngBsz1EsIjLAEkmnI5agI5agobmDrfvaiSWStHTE2dfWxWu7WumIJ3i5oYUt+9qIJTL/xt19ge6MJ5k1tpqueJLKkigjK8soihjH11ZSWhRh4sgKYvEkY2vKKI5GDr4vGjG6EklGVZZQVVpMJAIjK0soLYpSWhShtChCRUlRcCGPGGZ9fa8dOnKZKBYBN5vZvcDrgGZ3/5vbTiKSXbFEkrauBPvbY2xraqcjlqAznqQjlqCpLUZbV4L2WIKmti7au4Jt63a1MryimPZYgrU7WjiQwTf5iEHSYf7xI5k+poq66lKiZkwdXUVRJEJx1CgpihBPOBNGlFNeEqW2qpS66lKKo3qyP5tCSxRmdg9wAVBrZluBfwaKAdz9DuBR4HJgHdAGvDusWESGInenM56kobmDhuZ22rsSdMSCi3pDczvF0Qjrd7dSFImwdmcLDc0dR3X8suIIIypKKCuOUhKN8FpjKzPGVHPW1FqS7tRVlTKlLvhmP7yihNqqUsbWlFJWHFzwy4qjIf3lMtBCSxTuft0RtjvwobA+X2QwcXfaYwn2HuiiqS3G7tZO2rqCWzpRg4172tiyt41IxGjpiPHC5iZiiSRH6j+tH17O/vYYs8ZVUz+8nAkjyplaV0VZcZR40plcW9Hjwh6huqyYYWXFlBZFiESG9u2YoSSXt55EhCAJtHTGeW1XK6/ubCXhzssN+2ntTLC9qZ0t+9rY0dxBPHnke/hlxRFOHl/DG08YQyRizBpbjbszY0w1ZcVRxgwro7w4yrjhZbp9IxlTohAJUfDETifbm9pZu7OFLfvaMWDl9mYamjtIJp3tzR10xZN9vn/OhBpmT6hhwUljqa0uJZ5IMqWuiuJohHE1ZVSURBlRUUJ5SdAJO9Q7XSUcShQix6iprYste9tZ07CfbU3t/OHlnew7EHQEH07whE2E0cPKuGDmaOqqS6ksiTJr3DAmjaygqqyIYWXFWfwrRA5PiUIkAy0dMTal+gGeWb+HxpZOmttjrN3Zwu7Wrr/at7aqFIA3njiG6aOrqCwtYuLICkZXl3LCuGEMKyvSN38pKEoUIj3EEkmWbNzLyw0tvLB5H6u372f97gN97jtxZDmnHzeCU+prmDiygpryYk4/bgTVagnIIKNEIUNaMuks39bM06/t5r6lW9nQKymMrynjolmjGVtTxonjhnHCuGqm1FYxorIkRxGLZJ8ShQwZ+zti/HHNLv748i6e37Tvb/oQ6qpLOWdaLefNqOXsqbVMG12lZ/1FUKKQQWzdrlYeW7WDTXsOsHTjvr+6hVQ/vJzzZtRRV1XKCeOqufSksUwcWZHDaEXylxKFDAotHTEeWd7Aa42tbNzTxh/W7KTnsIOT64dx1Wn1XHzCaM6eWstI3ToSyZgShRScA51xlmzcyzPr97ByWzMrtjazvyN+cHt1WRGvn1ZLaVGEj1w8nZPG1xDVKGKRflOikLzl7mze28aahv28sLmJDbsP8MTaxr8anFYUMUZXl3L21FpeP72WK08dr/EHIgNMiULySkcswc+e3cQDL2xjdcP+g+vNYHxNOWdNGcWEEeWccfxIzp9Rp6ePRLJAiUJy6slXG3nmtT08+epudrV0sHN/JwDDK4p515nHMbamjBljqjl3eq2eQBLJESUKySp359Vdrazc1szDyxv448u7AA5WLb3y1Homjijn2jMmUVKkonUi+UCJQkLl7qzavp8n1jby2KodLN/afHBbTXkxfzdnPB+9eDrTRlflMEoRSUeJQgZURyzB85v28cTaRpZu3MsLm5sObhtWVsTUukqumD2e82bUcXL9MEqLdDtJJN8pUcgxc3d+vWwb9/xlC89t3HtwfUk0wrnTa5l33EjefOp4JtdW5jBKEekvJQrpl3giySMrGrj/+a0s29xES2cwjmHOhBouP2UcV51Wz+hhZTmOUkQGghKFZCyeSLL41UbufHIDT7+25+D682bUce60Wq6YM45xNeU5jFBEwqBEIWntO9DFL1/YylPrdvPE2saDZTGOH1XBu846nmvPmEhVqf4ZiQxm+j9c/sYjyxv42bOb2N7czqY9bQBEDC6cOZpzptfyllPrNdBNZAhRohAAmttiPLZ6B3c/tfHgiOiZY6r5yMXTef3UUZx+3AiKohrXIDIUKVEMYbFEkt+v3slDy7fzm5U7cIfaqhLee85kPrVgph5dFRFAiWJI+vOru/nW7175qzEOJ40fxicuncn50+uIqNKqiPSgRDGErNvVwud+vZJn1wdjHc6cMpI3zR6viqsikpYSxSDXGU/whzW7+MKiVexqCQruXf+6SbzvnMlMqVPZDBE5MiWKQaojluDLj6zhsVU7DiaIYWVFPHjzORohLSJHRYliEPrIPS+y6KXtB5dvWTCLq0+vZ3S1RkqLyNFTohhEnny1kc/8agVb9rZTXVrErW85mStPHY+ZOqdFpP+UKAaB9Y2tfPL+5Ty/aR8Ab54zntuuma3HW0VkQChRFLCueJIvPrSK+5ZupSuR5P3nT+GD50+jpkJPMInIwFGiKEBNbV38Yc0uPnH/S7gHdZe+c/1cTq6vyXVoIjIIKVEUkC1727hv6RZ+sHg9nfEkVaVFvOus4/jUpTPVDyEioVGiKABNbV187L+X8T+vNAIwrqaMj71hBlfNradY9ZdEJGRKFHksmXR+/PRGbn14NQBTaiv5+ttmM3fSCKIqsyEiWRJqojCzBcC3gShwp7t/rdf2GuBnwKRULN9w9x+HGVOhiCeSXPTNJ9i8t41JIyv4xKUzefOc8bkOS0SGoNAShZlFge8CbwS2AkvMbJG7r+6x24eA1e7+d2ZWB7xiZj93966w4ioE7s5H713G5r1tnDu9lp++Z776IEQkZ8JsUcwH1rn7egAzuxe4EuiZKByotuAqWAXsBeIhxpT3WjpiXP39p1m7s5WzpoxSkhCRnAuzJ7Qe2NJjeWtqXU/fAU4AtgMrgI+6e7L3gczsRjNbamZLGxsbw4o357Y3tfOm2//M2p2tXDRrNP/1f1+nJCEiORdmoujrCue9li8FlgHjgVOB75jZsL95k/tCd5/n7vPq6uoGOs688NBL27n4m0+wvamdr771FO664QwlCRHJC2HeetoKTOyxPIGg5dDTu4GvubsD68xsAzALeC7EuPLOR+99kQeXbaeqtIj7bnq9Bs6JSF4Js0WxBJhuZpPNrAR4B7Co1z6bgYsBzGwMMBNYH2JMeec3Kxp4cNl2aqtK+fMtFypJiEjeCa1F4e5xM7sZeIzg8di73H2Vmd2U2n4HcCtwt5mtILhVdYu77w4rpnzz4uZ9fODnL1BZEuWBD5zN8IqSXIckIvI3Qh1H4e6PAo/2WndHj9fbgUvCjCEfJZPOFx5axU+f2QTAPTeeyaRRFTmOSkSkbxqZnWWxRJIbfvwcT63bA8CP/s88Zk8YntugRETSUKLIoub2GHO++DgAbzhhDHf8/VyKVKtJRPKcEkWWbNx9gEv+bTEA506v5Yf/cLoefxWRgqBEkQWb97Rx2befpCuR5CMXTePjl8zMdUgiIhlTogjZ6u37ufz2JwG47W2zuWbexCO8Q0Qkv+gGeYgeXLbtYJJ4//lTlCREpCCpRRGSjbsP8NF7lwHw43efwYUzR+c2IBGRflKLIgSd8QQXfONPgJKEiBQ+JYoB1t6V4KrvPg3A5aeMVZIQkYKnW08DKJ5IMv/Lv6elM86nFszkgxdMy3VIIiLHTC2KAXTtwmdp6YxTW1WqJCEig4YSxQC59eHVPL9pH1NqK1ny2YtzHY6IyIBRohgAv1i6hR/9eQNnThnJ4x87TyOuRWRQUaI4Rvs7Ynxx0SqiEePud89X7SYRGXTUmX0MYokks78QFPm764Z5lBVHcxyRiMjA09ffY/Cx/14GwHkz6rho1pjcBiMiEpKME4WZVYYZSKF5at1uHl7ewJTaSn7y7jNyHY6ISGiOmCjM7GwzWw2sSS3PMbPvhR5ZHosnknz8F8uoKi3inhvPVOe1iAxqmbQo/g24FNgD4O4vAeeFGVS++/Kja9i5v5Nb33ISY4aV5TocEZFQZXTryd239FqVCCGWgtDWFefHT21kdHUpV502IdfhiIiELpOnnraY2dmAm1kJ8BFSt6GGoq88Gvzp/3TJjBxHIiKSHZm0KG4CPgTUA1uBU4EPhhhT3trT2snPnt3MmVNGcu0Zk3IdjohIVmTSopjp7u/sucLMXg88FU5I+es9P1kKwFvn6paTiAwdmbQo/iPDdYNaQ3M7L21pYtroKt6umepEZAg5bIvCzM4CzgbqzOzjPTYNA4bcEOQvPxL0TXzpzSflOBIRkexKd+upBKhK7VPdY/1+4G1hBpVvFi5+jYeXN3DN6RM4e1ptrsMREcmqwyYKd38CeMLM7nb3TVmMKa88vmoHX3n0ZSbXVvLVt56S63BERLIuk87sNjO7DTgJODi6zN0vCi2qPJFMOjf+5/MA3HXDGaoMKyJDUiZXvp8DLwOTgS8CG4ElIcaUN36weD0A7z9vCpNrVepKRIamTBLFKHf/ERBz9yfc/T3AmSHHlXPLtjTx9d++zInjhvGpBbNyHY6ISM5kcusplvrdYGZvArYDg3ogQUcswdXffxqAb107h2hERf9EZOjKJFH8i5nVAP9EMH5iGPCPYQaVa998/BUSSeeTl85k1thhuQ5HRCSnjpgo3P3h1Mtm4EI4ODJ7UGps6eSHT25g7qThfOjCabkOR0Qk59INuIsCbyeo8fRbd19pZlcAnwHKgdOyE2J2ffPxVwC4br5qOYmIQPrO7B8B7wNGAbeb2Y+BbwD/6u4ZJQkzW2Bmr5jZOjP79GH2ucDMlpnZKjN74mj/gIGUTDr3LtlC/fByrlGZDhERIP2tp3nAbHdPmlkZsBuY5u47MjlwqkXyXeCNBFVnl5jZIndf3WOf4cD3gAXuvtnMRvfz7xgQdz21AYDr5itJiIh0S9ei6HL3JIC7dwBrM00SKfOBde6+3t27gHuBK3vtcz3wgLtvTn3OrqM4/oBKJJ2fPLMRgPeeMyVXYYiI5J10LYpZZrY89dqAqallA9zdZx/h2PVAz5nxtgKv67XPDKDYzP5EUE/q2+7+094HMrMbgRsBJk0Kp+/gkRUNbNnbzpeuPInykiFX81BE5LDSJYoTjvHYfQ0+8D4+/3TgYoIO8mfM7Fl3X/tXb3JfCCwEmDdvXu9jDIifPr2RipIo15yu204iIj2lKwp4rIUAtwI9r7oTCAbr9d5nt7sfAA6Y2WJgDrCWLFq2pYmlm/bxllPHqzUhItJLmFXulgDTzWxyaq7tdwCLeu3zIHCumRWZWQXBramsz8f9zw+uBOATl87M9keLiOS9TEZm94u7x83sZuAxgomO7nL3VWZ2U2r7He6+xsx+CywHksCd7r4yrJj6suil7by0tZn5k0cyYURFNj9aRKQgZJQozKwcmOTurxzNwd39UeDRXuvu6LV8G3Db0Rx3IN322MsAfPf6ubkKQUQkrx3x1pOZ/R2wDPhtavlUM+t9C6kg7T3QxZa97Zwwbhh11aW5DkdEJC9l0kfxBYIxEU0A7r4MOD6sgLLpc79eAcD/v+JYH/ASERm8MkkUcXdvDj2SLOuMJ3h0xQ5OnTics6dqHmwRkcPJpI9ipZldD0TNbDrwEeDpcMMK3+rt+wE4b0ZdjiMREclvmbQoPkwwX3Yn8F8E5cb/McSYsuIXS7cCcNnJY3MciYhIfsukRTHT3T8LfDbsYLJlW1M79zy3mavnTuCEcZqYSEQknUxaFN8ys5fN7FYzOyn0iLLgB0+8BsA7z9ScEyIiR3LEROHuFwIXAI3AQjNbYWafCzuwsLR2xnnghW1MG13FaROH5zocEZG8l1EJD3ff4e63AzcRjKn4fJhBhenxVTto7YzzgfOnYtZX3UIREekpkwF3J5jZF8xsJfAdgieeJoQeWUjufnojABfOyukcSSIiBSOTzuwfA/cAl7h77+qvBaehuYNpo6sYWVmS61BERArCEROFu5+ZjUCy4cXN+2hs6eSa0wu2QSQiknWHTRRm9gt3f7uZreCvJxzKdIa7vHPf88HYiWvmaXIiEZFMpWtRfDT1+4psBJIND7+0nWjEmFxbmetQREQKxmE7s929IfXyg+6+qecP8MHshDdw1u1qZX9HXCOxRUSOUiaPx76xj3WXDXQgYbv14dUAXD9fg+xERI5Guj6KDxC0HKaY2fIem6qBp8IObKA9v2kfoypLOHuaKsWKiByNdH0U/wX8Bvgq8Oke61vcfW+oUQ2wA51xWjvjnDR+ZK5DEREpOOkShbv7RjP7UO8NZjaykJLF7X98FYAF6p8QETlqR2pRXAE8T/B4bM96Fw5MCTGuAfXnV3cD8OY543MciYhI4TlsonD3K1K/J2cvnIHX3pVg1fb9XDd/IqOqNC+2iMjRyqTW0+vNrDL1+u/N7FtmVjCPDr2weR8Ap00ckeNIREQKUyaPx34faDOzOcCngE3Af4Ya1QC657nNAJw5ZVSOIxERKUyZJIq4uztwJfBtd/82wSOyBeH3a3YCMGlURY4jEREpTJlUj20xs/8HvAs418yiQHG4YQ2MA51xOmJJZo0tmLwmIpJ3MmlRXAt0Au9x9x1APXBbqFENkJ89uwmAd511XI4jEREpXJlMhboD+DlQY2ZXAB3u/tPQIxsADy4Lps+47oyC6XsXEck7mTz19HbgOeAa4O3AX8zsbWEHNhC2NbVTP7ycSERTnoqI9FcmfRSfBc5w910AZlYH/B64P8zAjpW709we44zjVbZDRORYZNJHEelOEil7MnxfTt2fmqRowojyHEciIlLYMmlR/NbMHiOYNxuCzu1HwwtpYCxOle349GWzchyJiEhhy2TO7E+a2VuBcwjqPS1091+FHtkxeuil7Zw9dRRlxdFchyIiUtDSzUcxHfgGMBVYAXzC3bdlK7Bjsb6xFYBZY4flOBIRkcKXrq/hLuBh4GqCCrL/kZWIBsCyLU0AXDFnXG4DEREZBNLdeqp29x+mXr9iZi9kI6CB8IeXd1FZEmV2fU2uQxERKXjpWhRlZnaamc01s7lAea/lIzKzBWb2ipmtM7NPp9nvDDNLDMT4jNbOOL9Z0cBZU2spiub9w1kiInkvXYuiAfhWj+UdPZYduCjdgVM1ob4LvBHYCiwxs0XuvrqP/b4OPHZ0offtW4+vJelw3fyJA3E4EZEhL93ERRce47HnA+vcfT2Amd1LUIF2da/9Pgz8EjjjGD+PZNK566kNzJlQw8UnjDnWw4mICOEOnKsHtvRY3ppad5CZ1QNXAXekO5CZ3WhmS81saWNj42H3656k6PJT1IktIjJQwkwUfRVY8l7L/w7c4u6JdAdy94XuPs/d59XV1R12v+6nnc6aqkmKREQGSiYjs/trK9Czo2ACsL3XPvOAe80MoBa43Mzi7v7r/nzgs+v3UlkS5RQ97SQiMmCOmCgsuIq/E5ji7l9KzZc91t2fO8JblwDTzWwysA14B3B9zx3cfXKPz7kbeLi/SQJgTcN+zpleSyrxiIjIAMjk1tP3gLOA61LLLQRPM6Xl7nHgZoKnmdYAv3D3VWZ2k5nd1M94D6sznmBbUztT6qoG+tAiIkNaJreeXufuc83sRQB332dmJZkc3N0fpVcBQXfvs+Pa3W/I5JiHs2H3AQDG1ZQdy2FERKSXTFoUsdRYB4eD81EkQ42qH/7jD+sAmDtpRI4jEREZXDJJFLcDvwJGm9mXgT8DXwk1qn5obO2kqrSIk9WRLSIyoDIpM/5zM3seuJjgkde3uPua0CM7Stub2vVYrIhICDKZM3sS0AY8BCwCDqTW5Y1k0tm6L5gfW0REBlYmndmPEPRPGFAGTAZeAU4KMa6jsvtAJ4AmKRIRCUEmt55O6bmcqhz7/tAi6ofV2/cDMGlkRY4jEREZfI66hIe7v8AAFPAbSKtSieJs9VGIiAy4TEZmf7zHYgSYCxy+Ml8OdNd4Or62MreBiIgMQpn0UVT3eB0n6LP4ZTjh9M/vVu9kxhiNyBYRCUPaRJEaaFfl7p/MUjxHbX9HDIDxeuJJRCQUh+2jMLOiVPnvjKY9zZUNjUHpjqvnTshxJCIig1O6FsVzBElimZktAu4DDnRvdPcHQo4tI2t3tgBQV12a40hERAanTPooRgJ7CObI7h5P4UBeJIq/bNgLwFRVjRURCUW6RDE69cTTSg4liG69Z6rLmS172wC1KEREwpIuUUSBKjKb0jRnGls6GTtMpcVFRMKSLlE0uPuXshZJP+1t6+LUicNzHYaIyKCVbmR23s8nGk8kaWqLMXGESneIiIQlXaK4OGtR9NNLW5sAGF5RnNtAREQGscMmCnffm81A+uPFzU0AXHLi2NwGIiIyiB11UcB88tDyBkqLIpxcPyzXoYiIDFoFmyiSSWftjhZOqa/BLO+7U0REClbBJoq9bV20xxJcOGt0rkMRERnUCjZR7NzfAUBVaSaDy0VEpL8KNlE0tgTTn6p0h4hIuAo2UWzd1w7A+OEalS0iEqaCTRS/X7MTgLE1ShQiImEq2ESxctt+KkuiVJSoj0JEJEwFmyjMNKudiEg2FGSicHcaWzo5a+qoXIciIjLoFWSiONCVAKAoUpDhi4gUlIK80m7aE8zIOnGkbj2JiIStIBPFS1uaAZg2WmMoRETCVpCJoise3HqaosF2IiKhK8hE0RFPAjCyoiTHkYiIDH6hJgozW2Bmr5jZOjP7dB/b32lmy1M/T5vZnEyO++dXdwNQUlSQeU5EpKCEdqU1syjwXeAy4ETgOjM7sdduG4Dz3X02cCuwMJNjV5ZGAYhGVF5cRCRsYX4lnw+sc/f17t4F3Atc2XMHd3/a3felFp8FJmRy4M54kjkTagY0WBER6VuYiaIe2NJjeWtq3eG8F/hNXxvM7EYzW2pmSxsbG1m2pYniqG47iYhkQ5hX277uC3mfO5pdSJAobulru7svdPd57j6vrq6OiBltqUF3IiISrjATxVZgYo/lCcD23juZ2WzgTuBKd9+TyYH3HujSYDsRkSwJM1EsAaab2WQzKwHeASzquYOZTQIeAN7l7mszOain2iSjq1VeXEQkG0Kr0e3ucTO7GXgMiAJ3ufsqM7sptf0O4PPAKOB7ZgYQd/d56Y4bTwaZQvNQiIhkR6iTObj7o8Cjvdbd0eP1+4D3Hc0x27rigMp3iIhkS8E9OuSpe0/HjarIcSQiIkNDwSWKrkSQKIaXq3yHiEg2FFyiiCWCOk815cU5jkREZGgouETR/dRTeUk0t4GIiAwRBZcouuJJ6jVXtohI1hRcojCD/R2xXIchIjJkFFyicIdZY6tzHYaIyJBReIkC1zwUIiJZVHBX3I5YkqJIwYUtIlKwCu6KWxQxmtvVRyEiki0Flygcle8QEcmmgksUsURSkxaJiGRRQV5xm9u7ch2CiMiQUZCJYsYYPR4rIpItBZkodOtJRCR7CvKKm0z2OfW2iIiEoCATxXjVehIRyZqCTBRFUct1CCIiQ0ZBJoqIKVGIiGRLQSaKERWa3U5EJFsKMlFEI2pRiIhkixKFiIikVaCJItcRiIgMHQV5yVVntohI9hRkotBwOxGR7CnIRFFdWpTrEEREhoyCTBSq9SQikj0FecXVU08iItlTkIlCfdkiItlTkIlCLQoRkewpzEShJoWISNYUZKIwJQoRkawpyEShW08iItlTkImisjSa6xBERIaMgkwUxZGCDFtEpCCFesU1swVm9oqZrTOzT/ex3czs9tT25WY2N5PjRnTrSUQka0JLFGYWBb4LXAacCFxnZif22u0yYHrq50bg+0c87gDHKSIi6YXZopgPrHP39e7eBdwLXNlrnyuBn3rgWWC4mY1Ld1A98SQikl1hVterB7b0WN4KvC6DfeqBhp47mdmNBC0OgE4zWzmwoRasWmB3roPIEzoXh+hcHKJzccjM/r4xzETR11f/3hXCM9kHd18ILAQws6XuPu/Ywyt8OheH6FwconNxiM7FIWa2tL/vDfPW01ZgYo/lCcD2fuwjIiI5FGaiWAJMN7PJZlYCvANY1GufRcA/pJ5+OhNodveG3gcSEZHcCe3Wk7vHzexm4DEgCtzl7qvM7KbU9juAR4HLgXVAG/DuDA69MKSQC5HOxSE6F4foXByic3FIv8+FuWtiUREROTwNcRYRkbSUKEREJK28TRRhlf8oRBmci3emzsFyM3vazObkIs5sONK56LHfGWaWMLO3ZTO+bMrkXJjZBWa2zMxWmdkT2Y4xWzL4f6TGzB4ys5dS5yKT/tCCY2Z3mdmuw4016/d1093z7oeg8/s1YApQArwEnNhrn8uB3xCMxTgT+Euu487huTgbGJF6fdlQPhc99vsjwcMSb8t13Dn8dzEcWA1MSi2PznXcOTwXnwG+nnpdB+wFSnIdewjn4jxgLrDyMNv7dd3M1xZFKOU/CtQRz4W7P+3u+1KLzxKMRxmMMvl3AfBh4JfArmwGl2WZnIvrgQfcfTOAuw/W85HJuXCg2oIaQFUEiSKe3TDD5+6LCf62w+nXdTNfE8XhSnsc7T6DwdH+ne8l+MYwGB3xXJhZPXAVcEcW48qFTP5dzABGmNmfzOx5M/uHrEWXXZmci+8AJxAM6F0BfNTdk9kJL6/067oZZgmPYzFg5T8GgYz/TjO7kCBRnBNqRLmTybn4d+AWd08M8gKSmZyLIuB04GKgHHjGzJ5197VhB5dlmZyLS4FlwEXAVOB3Zvaku+8PObZ806/rZr4mCpX/OCSjv9PMZgN3Ape5+54sxZZtmZyLecC9qSRRC1xuZnF3/3VWIsyeTP8f2e3uB4ADZrYYmAMMtkSRybl4N/A1D27UrzOzDcAs4LnshJg3+nXdzNdbTyr/ccgRz4WZTQIeAN41CL8t9nTEc+Huk939eHc/Hrgf+OAgTBKQ2f8jDwLnmlmRmVUQVG9ek+U4syGTc7GZoGWFmY0hqKS6PqtR5od+XTfzskXh4ZX/KDgZnovPA6OA76W+Scd9EFbMzPBcDAmZnAt3X2NmvwWWA0ngTncfdCX6M/x3cStwt5mtILj9cou7D7ry42Z2D3ABUGtmW4F/Borh2K6bKuEhIiJp5eutJxERyRNKFCIikpYShYiIpKVEISIiaSlRiIhIWkoUkpdSlV+X9fg5Ps2+rQPweXeb2YbUZ71gZmf14xh3mtmJqdef6bXt6WONMXWc7vOyMlUNdfgR9j/VzC4fiM+WoUuPx0peMrNWd68a6H3THONu4GF3v9/MLgG+4e6zj+F4xxzTkY5rZj8B1rr7l9PsfwMwz91vHuhYZOhQi0IKgplVmdkfUt/2V5jZ31SNNbNxZra4xzfuc1PrLzGzZ1Lvvc/MjnQBXwxMS73346ljrTSzf0ytqzSzR1JzG6w0s2tT6/9kZvPM7GtAeSqOn6e2taZ+/3fPb/iplszVZhY1s9vMbIkF8wS8P4PT8gypgm5mNt+CuUheTP2emRql/CXg2lQs16Zivyv1OS/2dR5F/kau66frRz99/QAJgiJuy4BfEVQRGJbaVkswsrS7Rdya+v1PwGdTr6NAdWrfxUBlav0twOf7+Ly7Sc1dAVwD/IWgoN4KoJKgNPUq4DTgauCHPd5bk/r9J4Jv7wdj6rFPd4xXAT9JvS4hqORZDtwIfC61vhRYCkzuI87WHn/ffcCC1PIwoCj1+g3AL1OvbwC+0+P9XwH+PvV6OEHdp8pc//fWT37/5GUJDxGg3d1P7V4ws2LgK2Z2HkE5inpgDLCjx3uWAHel9v21uy8zs/OBE4GnUuVNSgi+ifflNjP7HNBIUIX3YuBXHhTVw8weAM4Ffgt8w8y+TnC76smj+Lt+A9xuZqXAAmCxu7enbnfNtkMz8tUA04ENvd5fbmbLgOOB54Hf9dj/J2Y2naAaaPFhPv8S4M1m9onUchkwicFZA0oGiBKFFIp3EsxMdrq7x8xsI8FF7iB3X5xKJG8C/tPMbgP2Ab9z9+sy+IxPuvv93Qtm9oa+dnL3tWZ2OkHNnK+a2ePu/qVM/gh37zCzPxGUvb4WuKf744APu/tjRzhEu7ufamY1wMPAh4DbCWoZ/Y+7X5Xq+P/TYd5vwNXu/kom8YqA+iikcNQAu1JJ4kLguN47mNlxqX1+CPyIYErIZ4HXm1l3n0OFmc3I8DMXA29JvaeS4LbRk2Y2Hmhz958B30h9Tm+xVMumL/cSFGM7l6CQHanfH+h+j5nNSH1mn9y9GfgI8InUe2qAbanNN/TYtYXgFly3x4APW6p5ZWanHe4zRLopUUih+Dkwz8yWErQuXu5jnwuAZWb2IkE/wrfdvZHgwnmPmS0nSByzMvlAd3+BoO/iOYI+izvd/UXgFOC51C2gzwL/0sfbFwLLuzuze3mcYG7j33swdScEc4msBl4ws5XADzhCiz8Vy0sEZbX/laB18xRB/0W3/wFO7O7MJmh5FKdiW5laFklLj8eKiEhaalGIiEhaShQiIpKWEoWIiKSlRCEiImkpUYiISFpKFCIikpYShYiIpPW/3Z8gKTtbRBwAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# save the model and weights\n",
    "model_name = 'my_first_cnn_model'\n",
    "model_filepath = model_name + '.json'\n",
    "weights_filepath = model_name + '_weights.hdf5'\n",
    "\n",
    "model_json = model.to_json() # serialize model to JSON\n",
    "with open(model_filepath, 'w') as json_file:\n",
    "    json_file.write(model_json) \n",
    "\n",
    "\n",
    "# define the model checkpoint and Tensorboard callbacks\n",
    "checkpoint = ModelCheckpoint(weights_filepath, monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n",
    "tensorboard = TensorBoard(os.path.join('logs', model_name))\n",
    "callbacks_list = [checkpoint, tensorboard]\n",
    "\n",
    "\n",
    "# train the model\n",
    "train_steps = train_gen.n//train_gen.batch_size\n",
    "val_steps = val_gen.n//val_gen.batch_size\n",
    "\n",
    "history = model.fit(train_gen, steps_per_epoch=train_steps, \n",
    "                    validation_data=val_gen,\n",
    "                    validation_steps=val_steps,\n",
    "                    epochs=3,\n",
    "                    callbacks=callbacks_list)\n",
    "\n",
    "\n",
    "# ROC analysis\n",
    "\n",
    "val_prob = model.predict(val_gen)\n",
    "filenames=val_gen.filenames\n",
    "val_true_labels = []\n",
    "\n",
    "for i in filenames:\n",
    "    val_true_labels.append(int(i[0]))\n",
    "\n",
    "val_true_array = np.array(val_true_labels)\n",
    "\n",
    "val_prob_array = np.array(val_prob)\n",
    "\n",
    "val_true_array = val_true_array.reshape(16000,1)\n",
    "\n",
    "# Plotting the ROC curve\n",
    "\n",
    "fpr1 , tpr1 , thresholds = roc_curve(val_true_labels, val_prob)\n",
    "\n",
    "def plot_roc_curve(fpr1,tpr1): \n",
    "  plt.plot(fpr1,tpr1) \n",
    "  plt.axis([0,1,0,1]) \n",
    "  plt.xlabel('False Positive Rate') \n",
    "  plt.ylabel('True Positive Rate') \n",
    "  plt.show()    \n",
    "  \n",
    "plot_roc_curve (fpr1,tpr1) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "auc_score1 = auc(fpr1, tpr1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In case you are wondering why (for the first epochs in particular) the validation accuracy is much higher than the training accuracy and the validation loss is lower than the training loss, you can find your answer in the [Keras FAQ](https://keras.rstudio.com/articles/faq.html).\n",
    "\n",
    "### Before you start with the exercises...\n",
    "\n",
    "Make sure that you understand all new code components introduced in the example. Remember: you can always lookup the documentation of the used functions to get a better understanding about how they work.\n",
    "\n",
    "[This blog post](http://karpathy.github.io/2019/04/25/recipe/) by Andrej Kaprapthy with tips and tricks for training neural networks is highly recommended reading material.\n",
    "\n",
    "## Exercise 1\n",
    "\n",
    "The PatchCAMELYON challenge on Kaggle uses the area under the ROC curve as an evaluation measure. Describe the concept of ROC curve analysis and the area under the ROC curve (AUC) as an evaluation measure. Then, using methods from the sklearn Python toolbox (the required functions are alerady imported), perform ROC curve analysis and computation of the AUC for your model. \n",
    "\n",
    "Note that you will have to do this analysis on the validation set (since you do not have access to the ground truth for the test set). \n",
    "\n",
    "\n",
    "\n",
    "*When 1-specificity (i.e. False Positive Rate, FPR) on the x-axis is plotted against sensitivity (True Positive Rate, TPR) on the y-axis, the plot is called the Receiver Operator Characteristic (ROC) curve [A]. To compare different classifiers, i.e. the same classifier with different learning parameters or completely different classifiers, the ROC curve can be useful [boek]. A single run of one classifier gives a single point on the ROC plot [boek].* \n",
    "\n",
    "*In medical context for instance, the basic principle of the ROC curve is to quantify how accurately medical diagnostic tests can discriminate between two patient states e.g. diseased or healthy [A]. By altering the decision threshold we will get different fractions of TP and FP [B]. For all these decision thresholds then the fractions are plotted in the ROC curve. A classifier is deemed to be 'perfect' when it is located at the point (0,1) on the ROC curve, i.e. when the classifier has a TPR of 1 and a FPR of 0 for a specific decision threshold. Moreover, when the ROC curve corresponds to the 45 degree line (y = x), the diagnostic test that belongs to this curve is as good as random guessing (i.e. chance level) [A]. In other words, we have a test which yields positive or negative results unrelated to the true disease status [A]. Hence, the closer to the top-left-hand corner the result of a classifier is, the better it has performed [boek]. Thus, to compare different classifiers, one can use the Area Under the Curve (AUC) to evaluate and compare model performance. Since the closer the classifier gets to the 'ideal' point of (0,1), the larger the AUC and the better the classifier performs [boek].*\n",
    "\n",
    "\n",
    "[A] Hajian-Tilaki K. (2013). Receiver Operating Characteristic (ROC) Curve Analysis for Medical Diagnostic Test Evaluation. Caspian journal of internal medicine, 4(2), 627–635.\n",
    "[B] Charles E. Metz, Basic principles of ROC analysis, Seminars in Nuclear Medicine, Volume 8, Issue 4, 1978, Pages 283-298, ISSN 0001-2998,\n",
    "https://doi.org/10.1016/S0001-2998(78)80014-2.\n",
    "(https://www.sciencedirect.com/science/article/pii/S0001299878800142)\n",
    "[boek] Marsland, S. (2014). Machine Learning: An Algorithmic Perspective, Second Edition (Chapman & Hall/CRC Machine Learning & Pattern Recognition) (2de editie). Chapman and Hall/CRC.\n",
    "\n",
    "## Exercise 2\n",
    "\n",
    "It is possible to construct a neural network model that is equivalent to the model above, however, only using convolutional layers (i.e. without using any fully connected or \"dense\" layers). Construct and train such a model. \n",
    "\n",
    "What would be the advantage of only using convolutional layers?\n",
    "\n",
    "\n",
    "*Yes, it is possible to construct a neural network model equivalent to the previous model by using only convolutional layers. The difference between fully-connected and convolutional layers is that the neurons in a convolutional layer are connected only to a local region in the input and the sharing of weights by the neurons [D]. This sharing is done by convolving the input image / array with a kernel. Hence, in order to convert a fully connected layer to a convolutional layer with equivalent output, one has to pay attention to the size of the kernel used in the convolutional layer. That is, the kernel size has to be the same as the size of the input feature array and the filter size must be equal to the input volume [D] [E]. Consequently, the output will be equal to the output we received from a fully-connected layer [D]. In our particular case, this means that we do not flatten the array until after the convolutional layers since the kernels corresponding to the convolutional layers remain two dimensional. The two dense layers with respectively 64 and 1 neurons will be replaced by 2D convolutional layers with kernel size equal to the size of the input that the previous layer generates. From the model summary we find that the second max pooling layer gives an output shape of (None, 6, 6, 64) so the kernel of the first convolutional layer needs to be (6,6) with 64 filters. For the last convolutional layer we need only one output neuron, since we only want one final output from our model (i.e. the probability of the image belonging to class 1, binary problem). Hence, the last convolutional layer will have a kernel size of (1,1) with 1 filter (filter = 1 means the number of output filters in the convolution is equal to 1) [F]. To receive one probability value, we flatten the array after all the layers. To check if the neural network consisting of only convolutional layer is equivalent to the previous fully-connected neural network, we can examine if the amount of total parameters is the same. This is the case since for both the fully-connected as well as the fully convolutional neural network, the amount of total parameters equals 166,977. Hence we have created a network consisting of only convolutional layers that is equivalent to the first mentioned neural network.*\n",
    "\n",
    "\n",
    "*The architecture of a convolutional neural network, in comparison to an ordinary feed forward neural network, allows for better fitting to the image dataset caused by a reduction in the number of parameters involved and the reusability of weights by convolution with a kernel [C]. Using only convolutional layers drastically reduces the amount of weights that needs to be calculated and updated during back propagation. Usually, an advantage then would be that the computation time is decreased and overall the convolutional neural network model would be less computationally expensive. However, in our specific case, the convolutional neural network is equivalent to the fully-connected version since it has the same amount of parameters. Hence this advantage does not apply here. A benefit of using only convolutional layers that does apply here, is the ability to automatically identify relevant features without the need for human supervision [H]. CNN's are designed to automatically and adaptively spatial hierarchies of features, which makes them especially suitable for image recognition [I]. In conclusion, the higher efficiency of CNN's due to the usage of kernels to extract features anywhere in the image, gives convolutional layers a substantial advantage over fully connected layers regarding image processing [I].*\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "[C]  https://towardsdatascience.com/a-comprehensive-guide-to-convolutional-neural-networks-the-eli5-way-3bd2b1164a53\n",
    "\n",
    "[D] https://mein2work.medium.com/converting-fc-layers-to-conv-layers-8a43880a44ed\n",
    "\n",
    "[E] https://sebastianraschka.com/faq/docs/fc-to-conv.html#:~:text=Yes%2C%20you%20can%20replace%20a,exact%20same%20behavior%20or%20outputs.\n",
    "\n",
    "[F] https://keras.io/api/layers/convolution_layers/convolution2d/\n",
    "\n",
    "[G] https://towardsdatascience.com/types-of-convolution-kernels-simplified-f040cb307c37\n",
    "\n",
    "[H] https://journalofbigdata.springeropen.com/articles/10.1186/s40537-021-00444-8#:~:text=The%20main%20benefit%20of%20CNN,Recognition%20%5B79%5D%2C%20etc.\n",
    "\n",
    "[I] https://insightsimaging.springeropen.com/articles/10.1007/s13244-018-0639-9\n",
    "\n",
    "## Exercise 3\n",
    "\n",
    "Use the `kaggle_submission.py` file to prepare and submit results for evaluation to Kaggle. What is the result of the evaluation on the test set? How does it compare to the evaluation that you performed on the validation set?\n",
    "\n",
    "## Submission checklist\n",
    "\n",
    "* Exercise 1: Answer to the questions and code\n",
    "* Exercise 2: Answer to the questions and code\n",
    "* Exercise 3: Answer to the questions and code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notes\n",
    "\n",
    "Dense layers weghalen en dan in plaats daarvan andere layers toevoegen. model.flatten en dan twee dense layers. Een heeft 64 neuronen en de ander 1 omdat je 1 antwoord wilt. Die moet je dan weghalen, we willen beide dense layers weghalen omdat we naar convolutional willen. Nu op zoek gaan naar convolutional layers om de dense layers te vervangen. Met equivalent bedoelen ze meer de performance\n",
    "\n",
    "Goeie kernel en goeie neuronen voor de convolutional layers en dan vergelijken of er iets vergelijkbaars uitkomt. \n",
    "\n",
    "Wel weer eindigen met 1 neuron op het einde want we willen maar 1 output neuron. Kunnen we ook controleren --> summary van model plotten --> model.summary() --> dan krijgen we samenvatting van hele model --> per laag zien hoeveel parameters erinzitten. Evenveel parameters in convolutional als in fully connected.\n",
    "\n",
    "\n",
    "*Equivalent? Do they mean using a convolutional neural network with a kernel of 1 ? But what would be the advantage of that? Dense can be replaced by convolutional layer with kernel 1 and then we need to replace max pooling layer by convolutional layer?* \n",
    "\n",
    "\n",
    "*When using multiple convolutional layers, each convolutional layer is assigned their own features to detect. Usually, the first convolutional layer is used for low-level feature detection, e.g. edges, color, gradient orientation etc. [C]. The next layers then combine these features into more complex features to allow for the detection of specific characteristics of an image.* \n",
    "\n",
    "In summary, the goal of a convolutional neural network is to reduce the images into a form which allows for easy processing without losing features that are necessary for getting an accurate prediction [C].*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 144000 images belonging to 2 classes.\n",
      "Found 16000 images belonging to 2 classes.\n",
      "Epoch 1/3\n",
      "   1/4500 [..............................] - ETA: 0s - loss: 0.6931 - accuracy: 0.5000WARNING:tensorflow:From C:\\Users\\20192823\\.conda\\envs\\p361\\lib\\site-packages\\tensorflow\\python\\ops\\summary_ops_v2.py:1277: stop (from tensorflow.python.eager.profiler) is deprecated and will be removed after 2020-07-01.\n",
      "Instructions for updating:\n",
      "use `tf.profiler.experimental.stop` instead.\n",
      "4500/4500 [==============================] - ETA: 0s - loss: 0.4868 - accuracy: 0.7660\n",
      "Epoch 00001: val_loss improved from inf to 0.43426, saving model to model_fully_conv_weights.hdf5\n",
      "4500/4500 [==============================] - 3142s 698ms/step - loss: 0.4868 - accuracy: 0.7660 - val_loss: 0.4343 - val_accuracy: 0.8044\n",
      "Epoch 2/3\n",
      "4500/4500 [==============================] - ETA: 0s - loss: 0.4005 - accuracy: 0.8217\n",
      "Epoch 00002: val_loss improved from 0.43426 to 0.37792, saving model to model_fully_conv_weights.hdf5\n",
      "4500/4500 [==============================] - 471s 105ms/step - loss: 0.4005 - accuracy: 0.8217 - val_loss: 0.3779 - val_accuracy: 0.8372\n",
      "Epoch 3/3\n",
      "4500/4500 [==============================] - ETA: 0s - loss: 0.3643 - accuracy: 0.8401\n",
      "Epoch 00003: val_loss improved from 0.37792 to 0.34637, saving model to model_fully_conv_weights.hdf5\n",
      "4500/4500 [==============================] - 1057s 235ms/step - loss: 0.3643 - accuracy: 0.8401 - val_loss: 0.3464 - val_accuracy: 0.8501\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'  # or any {'0', '1', '2'}   \n",
    "import tensorflow as tf\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Flatten\n",
    "from tensorflow.keras.layers import Conv2D, MaxPool2D\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, TensorBoard\n",
    "import matplotlib.pyplot as plt\n",
    "# unused for now, to be used for ROC analysis\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "\n",
    "\n",
    "# the size of the images in the PCAM dataset\n",
    "IMAGE_SIZE = 96\n",
    "\n",
    "\n",
    "def get_pcam_generators(base_dir, train_batch_size=32, val_batch_size=32):\n",
    "\n",
    "     # dataset parameters\n",
    "     train_path = os.path.join(base_dir, 'train+val', 'train')\n",
    "     valid_path = os.path.join(base_dir, 'train+val', 'valid')\n",
    "\n",
    "\n",
    "     RESCALING_FACTOR = 1./255\n",
    "\n",
    "     # instantiate data generators\n",
    "     datagen = ImageDataGenerator(rescale=RESCALING_FACTOR)\n",
    "\n",
    "     train_gen = datagen.flow_from_directory(train_path,\n",
    "                                             target_size=(IMAGE_SIZE, IMAGE_SIZE),\n",
    "                                             batch_size=train_batch_size,\n",
    "                                             class_mode='binary')\n",
    "\n",
    "     val_gen = datagen.flow_from_directory(valid_path,\n",
    "                                             target_size=(IMAGE_SIZE, IMAGE_SIZE),\n",
    "                                             batch_size=val_batch_size,\n",
    "                                             class_mode='binary', shuffle = False) \n",
    "     # NB: first SHUFFLE = FALSE was not specified in the val_gen definition\n",
    "     # This was however specified in the code in the jupyter notebook\n",
    "     # This was the reason for the odd ROC curves\n",
    "\n",
    "     return train_gen, val_gen\n",
    "\n",
    "\n",
    "# Now we replace the last two dense layers with convolutional layers and examine the effect on model performance\n",
    "# by checking the ROC curves and loss curves on training and validation dataset\n",
    "# Since we do not have the ground truth of the test set, model performance will be evaluated on the test set.\n",
    "\n",
    "def get_model_fullyconv(kernel_size=(3,3), pool_size=(4,4), first_filters=32, second_filters=64):\n",
    "\n",
    "\n",
    "     # build the model\n",
    "     model = Sequential()\n",
    "\n",
    "     model.add(Conv2D(first_filters, kernel_size, activation = 'relu', padding = 'same', input_shape = (IMAGE_SIZE, IMAGE_SIZE, 3)))\n",
    "     model.add(MaxPool2D(pool_size = pool_size)) \n",
    "\n",
    "     model.add(Conv2D(second_filters, kernel_size, activation = 'relu', padding = 'same'))\n",
    "     model.add(MaxPool2D(pool_size = pool_size))\n",
    "\n",
    "     model.add(Conv2D(64, (6,6), activation = 'relu'))\n",
    "     model.add(Conv2D(1, (1,1), activation = 'sigmoid'))\n",
    "     model.add(Flatten())\n",
    "\n",
    "    \n",
    "    \n",
    "     # compile the model\n",
    "     model.compile(SGD(learning_rate=0.01, momentum=0.95), loss = 'binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "     return model\n",
    "\n",
    "# get the model\n",
    "model_fullyconv = get_model_fullyconv()\n",
    "\n",
    "\n",
    "# get the data generators\n",
    "train_gen, val_gen = get_pcam_generators('C:/Users//20192823//Documents//3 jaar//Kwartiel 3//BIA')\n",
    "\n",
    "\n",
    "\n",
    "# save the model and weights\n",
    "model_name = 'model_fully_conv'\n",
    "model_filepath = model_name + '.json'\n",
    "weights_filepath = model_name + '_weights.hdf5'\n",
    "\n",
    "model_json = model_fullyconv.to_json() # serialize model to JSON\n",
    "with open(model_filepath, 'w') as json_file:\n",
    "    json_file.write(model_json)\n",
    "\n",
    "\n",
    "# define the model checkpoint and Tensorboard callbacks\n",
    "checkpoint = ModelCheckpoint(weights_filepath, monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n",
    "tensorboard = TensorBoard(os.path.join('logs', model_name))\n",
    "callbacks_list = [checkpoint, tensorboard]\n",
    "\n",
    "\n",
    "# train the model\n",
    "train_steps = train_gen.n//train_gen.batch_size\n",
    "val_steps = val_gen.n//val_gen.batch_size\n",
    "\n",
    "history = model_fullyconv.fit(train_gen, steps_per_epoch=train_steps,\n",
    "                    validation_data=val_gen,\n",
    "                    validation_steps=val_steps,\n",
    "                    epochs=3,\n",
    "                    callbacks=callbacks_list)\n",
    "\n",
    "# ROC analysis\n",
    "\n",
    "val_prob = model_fullyconv.predict(val_gen)\n",
    "filenames=val_gen.filenames\n",
    "val_true_labels = []\n",
    "\n",
    "for i in filenames:\n",
    "    val_true_labels.append(int(i[0]))\n",
    "\n",
    "val_true_array = np.array(val_true_labels)\n",
    "\n",
    "val_prob_array = np.array(val_prob)\n",
    "\n",
    "val_true_array = val_true_array.reshape(16000,1)\n",
    "\n",
    "# Plotting the ROC curve\n",
    "\n",
    "fpr , tpr , thresholds = roc_curve(val_true_labels, val_prob)\n",
    "\n",
    "def plot_roc_curve(fpr,tpr): \n",
    "  plt.plot(fpr,tpr) \n",
    "  plt.axis([0,1,0,1]) \n",
    "  plt.xlabel('False Positive Rate') \n",
    "  plt.ylabel('True Positive Rate') \n",
    "  plt.title('ROC curve - model with only convolutional layers')\n",
    "  plt.show()    \n",
    "  \n",
    "plot_roc_curve (fpr,tpr) \n",
    "auc_score = auc(fpr, tpr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9312683828125"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "auc_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8903727499999999"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "auc_score1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Based on the AUC score, it is seen that the model with only convulational layers is more accurate at discrimination, with a value of 0.931, versus a value of 0.890. Comparison of the graphs shows a similar result.    *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_roc_curve(fpr,tpr,fpr1,tpr1): \n",
    "  plt.plot(fpr,tpr) \n",
    "  plt.plot(fpr1,)\n",
    "  plt.axis([0,1,0,1]) \n",
    "  plt.xlabel('False Positive Rate') \n",
    "  plt.ylabel('True Positive Rate') \n",
    "  plt.title('ROC curve - model with only convolutional layers')\n",
    "  plt.show()    \n",
    "  \n",
    "plot_roc_curve (fpr,tpr)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
